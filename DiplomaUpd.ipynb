{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DiplomaUpd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMQe4JEQVlCrvPRrRdvg7e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewDiv/PMfMIVV/blob/main/DiplomaUpd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "0sPgdJhm4yCD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VKwhxMSGMDRL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from scipy import optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulator"
      ],
      "metadata": {
        "id": "Quz4tRXI41nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Prevalence(host_num, single_strain_prob = 0.3):\n",
        "  \"\"\" Simulate prevalence of strains in the host\n",
        "    \n",
        "  Parameters\n",
        "  ----------\n",
        "  host_num : int\n",
        "    Number of patients\n",
        "  single_strain_prob : float\n",
        "    The probability that the host carries only one strain\n",
        "       \n",
        "  Returns\n",
        "  -------\n",
        "  prevs: ndarray, shape (host_num,)\n",
        "    List of prevalence of one strain\n",
        "  \"\"\"\n",
        "  prevs = np.zeros(host_num)\n",
        "  for i in range(host_num):\n",
        "    rnd = np.random.uniform()\n",
        "    if rnd < single_strain_prob:\n",
        "      prevs[i] = np.random.choice([0., 1.])\n",
        "    else:\n",
        "      prevs[i] = np.random.uniform()\n",
        "  return prevs"
      ],
      "metadata": {
        "id": "KfhWNVM1MMUE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pi_i(r_s, prevs, er_rate):\n",
        "  return prevs*(r_s[0]*(1 - er_rate) + (1 - r_s[0])*er_rate) + (1-prevs)*(r_s[1]*(1 - er_rate) + (1 - r_s[1])*er_rate)\n",
        "\n",
        "def SimulateReads(haps, prevs, coverage = 100,  er_rate = 0.01):\n",
        "  \"\"\" Simulate reads from one host\n",
        "    \n",
        "  Parameters\n",
        "  ----------\n",
        "  haps : ndarray, shape (n, m)\n",
        "    List of haplotypes\n",
        "  prevs: ndarray, shape (k,)\n",
        "    List of prevalences\n",
        "  coverage: int\n",
        "    Number of reads that cover sites\n",
        "  er_rate: float\n",
        "    Error rate \n",
        "       \n",
        "  Returns\n",
        "  -------\n",
        "  data: ndarray, shape (host_num, hap_len, hap_num)\n",
        "    Array of the number of observations of different alleles at each site\n",
        "     for all patients\n",
        "  \"\"\"\n",
        "  hap_len = len(haps[0])\n",
        "  hap_num = len(haps)\n",
        "  host_num = prevs.shape[0]\n",
        "  data = np.empty((host_num, hap_len, hap_num))\n",
        "  #print(hap_len, hap_num)\n",
        "  for host in range(host_num):\n",
        "    for snv in range(hap_len):\n",
        "      als = np.zeros(hap_num)\n",
        "      lcoverage = coverage\n",
        "      als[1] = np.random.binomial(lcoverage, pi_i([haps[0][snv], haps[1][snv]], prevs[host], er_rate))\n",
        "      als[0] = lcoverage - als[1]\n",
        "      data[host][snv] = als\n",
        "    \n",
        "  return data"
      ],
      "metadata": {
        "id": "AtYBeZ_8kfmb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log-Likelihood"
      ],
      "metadata": {
        "id": "QkdjgmuM4-_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogFactorial(n):\n",
        "  return sum(np.log(np.arange(1, n+1)))\n",
        "\n",
        "def Coeff(reads):\n",
        "  coeff = 0\n",
        "  for _reads in reads:\n",
        "    coeff += LogFactorial(_reads[0] + _reads[1]) - LogFactorial(_reads[1]) - LogFactorial(_reads[0])\n",
        "  return coeff"
      ],
      "metadata": {
        "id": "IP1Lq2cR1gHK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SingleHostLLH(prev, haps, samp_reads, er_rate=0.):\n",
        "  \"\"\" Calculate minus log-likelihood of a single host\n",
        "    \n",
        "  Parameters\n",
        "  ----------\n",
        "  prev : float\n",
        "    prevalence of 1st strain\n",
        "  haps : ndarray, shape (hap_num, m)\n",
        "    List of haplotypes\n",
        "  samp_reads: ndarray, shape (m, hap_num)\n",
        "    reads of host\n",
        "  er_rate: float\n",
        "    Error rate \n",
        "       \n",
        "  Returns\n",
        "  -------\n",
        "  -llh: float\n",
        "    log-likelihood of a single host\n",
        "  \"\"\"\n",
        "  llh = Coeff(samp_reads)\n",
        "  for snv in range(len(samp_reads)):\n",
        "    p = prev*(haps[0][snv]*(1.0 - er_rate) + (1.0 - haps[0][snv])*er_rate) + (1-prev)*(haps[1][snv]*(1.0 - er_rate) + (1.0 - haps[1][snv])*er_rate)\n",
        "    if p <= 0:\n",
        "      print([haps[i][snv] for i in range(len(prev))])\n",
        "      print(\"error rate: \", er_rate) \n",
        "      print(\"prevs:\", prev)\n",
        "    \n",
        "    llh += samp_reads[snv][1] * np.log(p) + samp_reads[snv][0] * np.log(1.0 - p)\n",
        "  return -llh\n",
        "\n",
        "def LLH(haps, prevs, samples, num_of_haplotypes, er_rate):\n",
        "  \"\"\" Calculate minus log-likelihood of the hosts\n",
        "    \n",
        "  Parameters\n",
        "  ----------\n",
        "  haps : ndarray, shape (hap_num, m)\n",
        "    List of haplotypes\n",
        "  prevs: ndarray, shape (k,)\n",
        "    List of prevalences of 1st strain\n",
        "  samples: ndarray, shape (n, m, hap_num)\n",
        "    reads of all hosts\n",
        "  num_of_haplotypes: int\n",
        "    hap_num\n",
        "  er_rate: float\n",
        "    Error rate \n",
        "       \n",
        "  Returns\n",
        "  -------\n",
        "  -llh: float\n",
        "    log-likelihood of the whole sample\n",
        "  \"\"\"  \n",
        "  haps = np.reshape(haps, (num_of_haplotypes, -1))\n",
        "  #print(haps)\n",
        "  llh = 0\n",
        "  for sample_num in range(len(samples)):\n",
        "      llh += SingleHostLLH(prevs[sample_num], haps, samples[sample_num], er_rate)\n",
        "  #print('LLH:', llh)\n",
        "  return llh"
      ],
      "metadata": {
        "id": "LVkhnhbd4wEk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimise_prevs_const_haps(haps, prevs, samp_reads, hap_num=2, er_rate=0.):\n",
        "  haps = np.reshape(haps, (hap_num, -1))\n",
        "  #prevs = np.reshape(prevs, (len(samp_reads), -1))\n",
        "  #print(prevs.shape)\n",
        "  llh = 0.\n",
        "  opt_prevs = np.empty(len(prevs))\n",
        "  bnds = ((0, 1),)\n",
        "  for host_num in range(len(samp_reads)):\n",
        "    opt_res = optimize.minimize(SingleHostLLH, prevs[host_num], args=(haps, samp_reads[host_num], er_rate),\n",
        "                                method='trust-constr', bounds=bnds, options = {'xtol': 1e-6, 'gtol': 1e-6, 'barrier_tol': 1e-6, 'maxiter': 100})\n",
        "    opt_prevs[host_num] = opt_res.x\n",
        "    llh += opt_res.fun\n",
        "    \n",
        "  return opt_prevs"
      ],
      "metadata": {
        "id": "SFHuO8MGSsQf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimise_haps_const_prevs(haps, prevs, samp_reads, num_of_hapl=2, er_rate=0.):\n",
        "  #print(haps)\n",
        "  prevs = np.reshape(prevs, (len(samp_reads), -1))\n",
        "  bnds = ((0, 1), ) * len(haps)\n",
        "  opt_res = optimize.minimize(LLH, haps, args=(prevs, samp_reads, num_of_hapl, er_rate),\n",
        "                                method='trust-constr', bounds=bnds, \n",
        "                                options = {'xtol': 1e-6, 'gtol': 1e-6, 'barrier_tol': 1e-6, 'maxiter': 100})\n",
        "  opt_haps = opt_res.x\n",
        "\n",
        "  return opt_haps"
      ],
      "metadata": {
        "id": "Io6NLK66NaWz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "DSG7npvtqSWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exmp_haplotypes = [[1., 1., 0., 0.],\n",
        "              [0., 0., 1., 1.]] # Probabilistic description of haplotypes\n",
        "\n",
        "hap_len = len(exmp_haplotypes[0]) # Number of sites\n",
        "coverage = 100 # coverage sites\n",
        "hap_num = len(exmp_haplotypes)  # Number of haplotypes(strains)\n",
        "samp_num = 10  # Number of samples(patients)\n",
        "er_rate = 0.01  # Error rate"
      ],
      "metadata": {
        "id": "Ld-JumM1qVcE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exmp_prevs = Prevalence(samp_num, 0.3)\n",
        "print(exmp_prevs)\n",
        "\n",
        "patients_data = SimulateReads(exmp_haplotypes, exmp_prevs, er_rate= 0.)\n",
        "print(patients_data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH24SxCqqmJK",
        "outputId": "aa50448b-7d96-4660-e65a-bb7fd93cc6d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35571709 0.         0.29360537 0.66695958 0.79481425 0.\n",
            " 0.57836888 0.14282305 0.36709016 0.06722632]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2022)\n",
        "start_time = time.time()\n",
        "\n",
        "haps_init = np.random.uniform(size = hap_num * hap_len)\n",
        "prev_init = np.random.uniform(size = samp_num)\n",
        "\n",
        "\n",
        "opt_haps = optimise_haps_const_prevs(haps_init, prev_init, patients_data)\n",
        "opt_prevs = optimise_prevs_const_haps(opt_haps, prev_init, patients_data)\n",
        "\n",
        "for _ in range(6):\n",
        "    opt_haps = optimise_haps_const_prevs(opt_haps, opt_prevs, patients_data)\n",
        "    opt_prevs = optimise_prevs_const_haps(opt_haps, opt_prevs, patients_data)\n",
        "\n",
        "print(\"time elapsed: {:.2f}s\".format(time.time() - start_time))\n",
        "\n",
        "print('RESULT')\n",
        "print(\"opt_haps:\", np.reshape(opt_haps, (hap_num, -1)))\n",
        "print(\"real_haps:\", exmp_haplotypes)\n",
        "print(\"opt_prevs:\", opt_prevs)\n",
        "print(\"real_prevs:\", exmp_prevs)"
      ],
      "metadata": {
        "id": "rjQPap3Kqr1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simultaneous optimisation"
      ],
      "metadata": {
        "id": "RlcQD6R0NxqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3wpnwTgrK8TV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}